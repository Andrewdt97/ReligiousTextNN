{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Religious Affiliation with Machine Learning\n",
    "### By Andrew Thomas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification problems are a mainstay in machine learning, but many are done with relatively boring datasets by my standards. While plenty of useful information can be extracted from movie or product reviews, the potential classification from these did not seem very interesting. I love religious writings because understanding a person’s metaphysics is important to knowing them well. I was wondering what classification could be done with religious texts and the most obvious seemed to be if a neural network could determine the religious affiliation of a piece of text. I see this as an interesting problem mostly for the muddled examples. For instance, American thought (including religious) is being more and more influenced by an Eastern worldview including their religious outlooks. Could a neural network find these influences? For instance, could an article on Christianity containing strong buddhist ideas or themes be detected? I was probably naive to think that could be constructed within the scope of this project, but I was able to produce a neutral network that attempts to find religious affiliation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement my solution, I built a RNN using Keras as a front end with Tensorflow as the back end. To help with preprocessing, I used [NLTK](https://www.nltk.org/) and [SKLearn](https://scikit-learn.org/stable/). NLTK is a natural language processing library that helps with the tokenization of input strings. SKLearn is a helpful machine learning library that I used to help initialize my training and test data sets. Being only text and of limited quantity, the dataset is included in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implements my solution, I hand crafted a dataset from various sources. I pulled writings from Hinduism, Buddhism, Christianity, Islam, and affiliation. I copied these into folder-organized text files named after their source. I grabbed the holy books from the religions as well as news articles, commentaries, and blog posts. In total, there are 980 texts. Many files contain more than one piece of writing, in such cases, text is delimited by three dollar signs.\n",
    "\n",
    "The first piece of the operation is to get labeled data. My python script scrapes through the textfiles and stores the name of the folder it was in as a label with the text in a document object. In the process of scraping, all punctuation (aside from dashes) is removed. ~~as well as stop words. Stop words are common words like “is” or “and” that don’t add much meaning to sentence.~~ While stop words were originally omitted, their reintroduction improved performance, see results for more. With the texts separated, they are put into a tokenizer, which creates a dictionary to allow for encoding. Once this tokenizer is created, the data is encoding as a sequence of integers that map to strings in the tokenizer. The labels are also turned into digits using a keras method. Finally the model is ready to be trained.\n",
    "\n",
    "While the dataset is not anemic by any means, it’s handmade nature is not gigantic either. Thus the architecture is rather simple to avoid overfitting. The NN starts by receiving the input vector and throws it into an embedding layer, which maps the text in a 32-dimensional space. From there a LSTM work on the text. Because the multi-dimensional output of the LSTM, the layers must be flattened before being put into the 5 softmax activated output nodes. The model is compiled using adam with  accuracy as a metric. In theory, categorical cross entropy should be the loss function, but in tests, binary_crossentropy preformed farm better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My results are nothing short of a story. I had obtained an accuracy of 92% without stop words. This was using the LSTM and a differently organized data set. In order to increase the size of my test set, I turned chapters/books of holy texts into their own entries rather than having multiple hundreds of pages run through the network. This dropped my accuracy down to around 85%. I was about to give up hope when I decided to re-include the stop words, thinking they may have an affect on the sequenced data. Sure enough, they did and my accuracy peaked at 95%. You would think that each religion would have buzz words to learn, but the model does much better when the stop words are included. I have no idea how to compare this, because I could not find anyone else attempting to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think my project has the potential for both good and harm. For one, as we become more globalized, if perfected, the tool could analyze the effect of religion on thought. Even if run on an unaffiliated piece of text, it could identify the influences of said writing. This could be useful for sociological study of philosophy work. However, with religious divides, it could be used to scrape the internet and marks individuals as targets for discrimination because of their religious affiliations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brownlee, Jason. \"How to Prepare Text Data for Deep Learning with Keras.\" *Machine Learning Mastery*. https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/. Accessed 12 May, 2019.\n",
    "\n",
    "Sinha, Nimesh. \"Understanding LSTM and its quick implementation in keras for sentiment analysis.\" *Towards Data Science*. https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47. Accessed 13 May, 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
